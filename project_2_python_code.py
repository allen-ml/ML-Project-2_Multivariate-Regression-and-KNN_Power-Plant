# -*- coding: utf-8 -*-
"""Project 2 Python Code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1--bgAt28Ih6_08lxvf36IGEgOUDEn3fG

#**Homework 2 - Power Plant**#

**Combined Cycle Power Plant Data Set** 

The dataset contains data points collected from a Combined Cycle Power Plant over  6 years (2006-2011), when the power plant was set to work with full load. Features consist of hourly average ambient variables Temperature (T), Ambient Pressure (AP), Relative Humidity (RH) and Exhaust Vacuum (V) to predict the net hourly electrical energy output (EP) of the plant. 

a) Download the Combined Cycle Power Plant data1 from: 
https://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant
"""

# Power Plant Data
# Import libraries

# Import Pandas 
import pandas as pd 
pd.__version__

# Import Numpy
import numpy as np

# Import Print Functions
from __future__ import print_function

# Import Seaborn
import seaborn as sns

import matplotlib.pyplot as plt


# Set the print options
pd.options.display.float_format = '${:,.2f}'.format

# Column Data Labels:  
# Features consist of hourly average ambient variables Temperature (AT), Ambient 
# Pressure (AP), Relative Humidity (RH) and Exhaust Vacuum (V) to predict the 
# net hourly power plant electrical energy output (PE) of the plant.

col_Names=["AT","V","AP","RH","PE"]

# Load data from Github - Public data, Raw
url = 'https://raw.githubusercontent.com/allen-ceo-ai/INFCourse/master/Folds5x2_pp_sheet1.csv'

# Read "comma delimited" data "Folds5x2_pp_sheet1" For Power Plant 
# Folds5x2_pp_sheet1 = pd.read_csv(url,names=col_Names)
Folds5x2_pp_sheet1 = pd.read_csv(url)

# Assign data to a dataframe
df1=pd.DataFrame(data=Folds5x2_pp_sheet1)
## df1=df1.iloc[1:]

# Print Column Names and data
print(df1.columns)
print(df1)

"""**b) Exploring the data:**

i) How many rows are in this data set? How many columns? What do the rows and columns represent?

ii) Make pairwise scatterplots of all the varianbles in the data set including the predictors (independent variables) with the dependent variable. Describe your findings. 

iii) What are the mean, the median, range, first and third quartiles, and in- terquartile ranges of each of the variables in the dataset? Summarize them in a table.
"""

print(df1.shape)

import seaborn as sns
plot_data=df1

# plot all features vs. the Powerpnat Energy output
f, axes = plt.subplots(1,4, sharey=True, figsize=(20, 4))
sns.scatterplot(x="AT", y="PE", data=plot_data, ax=axes[0])
sns.scatterplot(x="V", y="PE", data=plot_data, ax=axes[1])
sns.scatterplot(x="AP", y="PE", data=plot_data, ax=axes[2])
sns.scatterplot(x="RH", y="PE", data=plot_data, ax=axes[3])

# or alternatively use a for loop for ploting:
i=0
f, axes = plt.subplots(1,4, sharey=True, figsize=(20, 4))
for x_var in ["AT","V","AP","RH"]:
  sns.scatterplot(x_var, y="PE", data=plot_data, ax=axes[i])
  i=i+1

''' # Save the PNG file of the output graphs
from google.colab import files
plt.savefig("P2_Data_Exploration_Graph.png")
files.download("P2_Data_Exploration_Graph.png")'''

# or alternatively plot all at once:
sns.pairplot(data=plot_data,x_vars=["AT","V","AP","RH",],y_vars=["PE"])
pd.options.display.float_format = '${:,.2f}'.format

#Get the statistics of the data
df1.describe()

"""C) For each predictor, fit a simple linear regression model to predict the response. Describe your results. In which of the models is there a statistically significant association between the predictor and the response? Create some plots to back up your assertions. Are there any outliers that you would like to remove from your data for each of these regression tasks?"""

# Import Linear Regression and pyplot libraries
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt

# Perform regular Linear Regression Analysis on data
i=0
univariate_coef=np.array([[]])
f, axes = plt.subplots(1,4, sharey=True, figsize=(20, 4))
for x_var in ["AT","V","AP","RH"]:
  X_train=df1[[x_var]]
  Y_train=df1[["PE"]]
  reg = LinearRegression().fit(X_train,Y_train)
  Y_predict=reg.predict(X_train)
  Y_predict=pd.DataFrame(Y_predict)
  Y_predict.columns=["PE_reg"]
  df2 = pd.concat([X_train,Y_predict], axis=1)
  print(df2.head())
  print("Regression Coefficients for ",x_var,"are: ",reg.coef_)
  print("Regression intercept for ",x_var,"is: ",reg.intercept_)
  univariate_coef=np.append(univariate_coef,np.array([[reg.coef_]]))
  print(univariate_coef)
  plot_data=df1
  sns.scatterplot(x=x_var, y="PE", data=plot_data, ax=axes[i])
  plot_data=df2
  sns.lineplot(x=x_var, y="PE_reg", data=plot_data, ax=axes[i],color="red")
  i=i+1

"""Create some plots to back up your assertions. Are there any outliers that you would like to remove from your data for each of these regression tasks?"""

# Remove the features that don't seem to affect the output and dor the regression again - First the two contributing variables, second all variables.
from sklearn.linear_model import LinearRegression

# "AT and V are considered. AP and RH found to be outliers"
# Multivariate Regression using 2 features only
X_train=df1[["AT","V"]]
Y_train=df1[["PE"]]
reg = LinearRegression(normalize=True).fit(X_train,Y_train)
Y_predict=reg.predict(X_train)
Y_predict=pd.DataFrame(Y_predict)
Y_predict.columns=["PE_reg"]
#print(Y_predict)
df6 = pd.concat([X_train,Y_predict], axis=1)
print(df6.head())
print("Regression Coefficients for AT are",reg.coef_)
print("Regression Intercepts AT are",reg.intercept_)

# Multivariate Regression using all features
X_train=df1[["AT","V","AP","RH"]]
Y_train=df1[["PE"]]
reg = LinearRegression().fit(X_train,Y_train)
Y_predict=reg.predict(X_train)
Y_predict=pd.DataFrame(Y_predict)
Y_predict.columns=["PE_reg"]
#print(Y_predict)
df7 = pd.concat([X_train,Y_predict], axis=1)
print(df7.head())
print("Regression Coefficients for multivariate regression are",reg.coef_)
print("Regression Intercepts multivariate regression is",reg.intercept_)
multivariate_coef=np.array([[]])
multivariate_coef=reg.coef_
print('Polynomial coefficients based on univariet regression: ', univariate_coef)
print("The linear model is: Y = {:.5} + {:.5}X_1 + {:.5}X_2 + {:.5}X_3 + {:.5}X_4".format(reg.intercept_[0], reg.coef_[0][0], reg.coef_[0][1], reg.coef_[0][2], reg.coef_[0][3]))
print('Polynomial coefficients based on multivariable regression: ', multivariate_coef)
plt.scatter(univariate_coef,multivariate_coef, color='darkblue', marker='^')

"""**(f)** Is there evidence of nonlinear association between any of the predictors and the
response? To answer this question, for each predictor X, fit a model of the form
"""

# Create all nonlinear/polynomial combinations of states up to degree = 3 for your regression
from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree=3, interaction_only=False, include_bias=True)
univariate_coef_poly=np.array([[]])
for cols in ["AT","V","AP","RH"]:
    X_train=df1[[cols]]
    Y_train=df1[["PE"]]
    X=X_train
    
    # Transform your 4 column data to polynomial with interations - Degree = 3
    X_poly_train = pd.DataFrame(poly.fit_transform(X))
    print(X_poly_train)

    reg = LinearRegression().fit(X_poly_train,Y_train)
    Y_predict=reg.predict(X_poly_train)
    Y_predict=pd.DataFrame(Y_predict)
    Y_predict.columns=["PE_reg"]
    #print(Y_predict)
    df2 = pd.concat([X_poly_train,Y_predict], axis=1)
    print(df2.head())
    print("Regression Coefficients for this Column are:",reg.coef_)
    print("Regression Intercepts for this Column are: ",reg.intercept_)
    univariate_coef_poly=np.append(univariate_coef_poly,np.array([[reg.coef_]]))

print(univariate_coef_poly)

"""**(g)** Is there evidence of association of interactions of predictors with the response? To
answer this question, run a full linear regression model with all pairwise interaction
terms and state whether any interaction terms are statistically signifcant.

See this tutorial: https://towardsdatascience.com/the-complete-guide-to-linear-regression-in-python-3d3f8f06bf8
"""

# "AP" sounds to be correlated with the output. It is removed and the regression is performed again using polynomials
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
poly = PolynomialFeatures(degree=3, interaction_only=False, include_bias=True)
X_train=df1[["AT","V","RH"]]
Y_train=df1[["PE"]]
X=X_train
X_poly_train_all = pd.DataFrame(poly.fit_transform(X))
print(X_poly_train_all)
reg = LinearRegression().fit(X_poly_train_all,Y_train)
Y_predict=reg.predict(X_poly_train_all)
Y_predict=pd.DataFrame(Y_predict)
Y_predict.columns=["PE_reg"]
df2 = pd.concat([X_poly_train,Y_predict], axis=1)
print(df2.head())
print("Regression Coefficients for all pairewiseinteraction terms and states are: ",reg.coef_)
print("Regression Intercepts for all pairewiseinteraction terms and states are: ",reg.intercept_)
univariate_coef_poly_all=np.array([[reg.coef_]])
feature_names=pd.DataFrame(poly.get_feature_names())
print(feature_names)

"""What interactions are statistically significant?
p-value should be less than 0.05
"""

# Performing linear regression using statsmodels.api to see the R^2 and p values to decide what terms to eliminate. 
# Only univariate features are used for a toal of 4
from sklearn.metrics import r2_score
import statsmodels.api as sm
X = df1[["AT","V","AP","RH"]].to_numpy()
#X = df1[["AT","V"]].to_numpy()
y = df1[["PE"]].to_numpy()
X2 = sm.add_constant(X)
model = sm.OLS(y, X2)
results = model.fit()
print(results.summary())

# p-value should be less than 0.05

# All features and their interactions up to degree=2 are used
from sklearn.metrics import r2_score
import statsmodels.api as sm
X2 = sm.add_constant(X_poly_train_all)
model = sm.OLS(Y_train, X2)
results = model.fit()
print(results.summary())
#print(feature_names)
#selected_features=pd.DataFrame(feature_names.iloc[[2,8,9,10,11,14,16,18,19,21,23,24,25,26,27,28,29,30,33,34],[0]]) # this if for degree=3

# After reviewing results, certain features with low p-value are removed. 
# The remaining features are as follows:
selected_features=pd.DataFrame(feature_names.iloc[[0,2,5,8,11,13,14],[0]])
print(selected_features)

# Performing the liner regression on the selected polynomial features.
# X_train_selected=X_poly_train_all[[2,8,9,10,11,14,16,18,19,21,23,24,25,26,27,28,29,30,33,34]] # this if for degree=3
X_train_selected=X_poly_train_all[[0,2,5,8,11,13,14]]
X2 = sm.add_constant(X_train_selected)
y = df1[["PE"]]
model = sm.OLS(y, X2)
results = model.fit()
print(results.summary())

"""Also, run a regres-sion model involving all possible interaction terms and quadratic nonlinearities,and remove insigni
cant variables using p-values (be careful about interaction
terms).
"""

from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=True)
X_train=df1[["AT","V","AP","RH"]]
Y_train=df1[["PE"]]
X=X_train
X_poly_train_all = pd.DataFrame(poly.fit_transform(X))
feature_names=pd.DataFrame(poly.get_feature_names())
print(feature_names)

from sklearn.metrics import r2_score
import statsmodels.api as sm
X2 = sm.add_constant(X_poly_train_all)
model = sm.OLS(Y_train, X2)
results = model.fit()
print(results.summary())

selected_features=pd.DataFrame(feature_names.iloc[[0,1,2,3,4,5,6,8,10,12,13,14],[0]])
print(selected_features)

X_train_selected=X_poly_train_all[[0,1,2,3,4,5,6,8,10,12,13,14]]
X2 = sm.add_constant(X_train_selected)
y = df1[["PE"]]
model = sm.OLS(y, X2)
results = model.fit()
print(results.summary())

"""Train the regression model on a
randomly selected 70% subset of the data with all predictors. Test both models on the remaining points and report your train and test
MSEs.
"""

# Used 70% of data for training and 30% for testing 
from sklearn.model_selection import train_test_split
Y_train=df1[["PE"]]
X_train, X_test, y_train, y_test = train_test_split(X_poly_train_all, Y_train, test_size=0.3, random_state=42,shuffle=False)
print(X_poly_train_all)
X_train=pd.DataFrame(X_train.reset_index(drop=True))
X_test=pd.DataFrame(X_test.reset_index(drop=True))
y_train=pd.DataFrame(y_train.reset_index(drop=True))
y_test=pd.DataFrame(y_test.reset_index(drop=True))

print('X_train: \n',X_train)
print('y_train: \n',y_train)
print('X_test: \n',X_test)
print('y_test: \n',y_test)

# Ran liner regression on all the variable using a LinearRegression library
from sklearn.linear_model import LinearRegression
reg = LinearRegression(normalize=True).fit(X_train,y_train)
y_predict=reg.predict(X_test)
y_predict=pd.DataFrame(y_predict)
y_predict.columns=["PE_reg"]
print(X_test)
print(y_test)
print(y_predict)
#print(Y_predict)
df6 = pd.concat([X_test,y_predict], axis=1)
print(df6)
print("Regression Coefficients for all are",reg.coef_)
print("Regression Intercepts all are",reg.intercept_)

from sklearn.metrics import mean_squared_error
print("MSE of the regression using all states and interactions is: ",mean_squared_error(y_test, y_predict))

import matplotlib.pyplot as plt
plt.plot(y_test,y_predict,'o', color='black')

# Performing the liner regression on the selected polynomial features.
from sklearn.model_selection import train_test_split
Y_train=df1[["PE"]]
X_train_selected=X_poly_train_all[[0,1,2,3,4,5,6,8,10,12,13,14]]
X_train, X_test, y_train, y_test = train_test_split(X_train_selected, Y_train, test_size=0.3, random_state=42,shuffle=False)
X2 = sm.add_constant(X_train)
model = sm.OLS(y_train, X2)
results = model.fit()
print(results.summary())

y_predict=results.predict(X_test)
from sklearn.metrics import mean_squared_error
print("MSE of the regression is: ",mean_squared_error(y_test, y_predict))

import matplotlib.pyplot as plt
plt.plot(y_test,y_predict,'o', color='black')

"""**i) KNN Regression:**

i. Performk-nearest neighbor regression for this dataset using both normalized
and raw features. Find the value of k: 1,2, ...,100 that gives you the
best fit. Plot the train and test errors in terms of 1/k.
"""

# Performing the liner regression using K Neighbors Regressor with changing K
# Import metrics model to check the accuracy 
from sklearn import metrics

k_range=range(1,100+1,1)
#for n in k_range:
#   print(n)

MSE = {}
MSE_list = []

X_KNR=df1[["AT","V","AP","RH"]]
Y_KNR=df1[["PE"]]

Y_train=df1[["PE"]]
X_train, X_test, y_train, y_test = train_test_split(X_KNR, Y_KNR, test_size=0.3, random_state=42,shuffle=False)

##print(X_train)
#print(Y_train)

for k in k_range:
  n_neighbors=k
  # import k-nearest neighbor regression library
  from sklearn.neighbors import KNeighborsRegressor
  #KNR = KNeighborsRegressor(n_neighbors=7,weights='uniform')
  KNR = KNeighborsRegressor(n_neighbors,weights='uniform')
  KNR.fit(X_train, y_train)
  y_pred = KNR.predict(X_test)
  y_pred=pd.DataFrame(y_pred)
  y_pred.columns=["PE_reg"]
  from sklearn.metrics import mean_squared_error
  test_error=mean_squared_error(y_test, y_pred)
  #print("MSE of the regression is: ",mean_squared_error(y_test, y_pred))
  MSE_list.append(mean_squared_error(y_test, y_pred))

import matplotlib.pyplot as plt

#Plot the relationship between K and the testing accuracy
plt.plot(k_range,MSE_list)
plt.xlabel('Value of K for KNR')
plt.ylabel('Testing MSE')
plt.show()

import matplotlib.pyplot as plt
plt.plot(y_test,y_pred,'o', color='black')
print('Maximum accuracy is achieved at minimum of MSE ',min(MSE_list))
print(MSE_list)

# Create a boolean list that locates where the score is maximum
bool_MSE_list= MSE_list == min(MSE_list)
print(bool_MSE_list)

# Print the values that match Boolean in the list compressed
from itertools import compress
print(list(compress(MSE_list, bool_MSE_list)))
# Find the locations of where the maximum is happening
min_locations=np.where(bool_MSE_list)[0]
print('Minimum MSE is happening at Locations: ',min_locations)